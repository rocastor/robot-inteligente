"""
ğŸ¤– MÃ³dulo de AnÃ¡lisis con IA - OPTIMIZADO
Maneja las consultas a OpenAI y el anÃ¡lisis de preguntas con procesamiento paralelo
"""

import asyncio
import json
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any
import openai
from openai import OpenAI
import random
import threading
import os

# Precios de GPT-4o-mini (por 1M tokens)
PRECIO_INPUT_POR_1M_TOKENS = 5.0
PRECIO_OUTPUT_POR_1M_TOKENS = 15.0

# Preguntas DEFAULT del sistema Robot AI (optimizadas para mÃ¡xima precisiÃ³n)
DEFAULT_QUESTIONS = [
    "Â¿CuÃ¡l es el nombre oficial de la entidad contratante o instituciÃ³n que estÃ¡ comprando o contratando? Responde ÃšNICAMENTE con el nombre de la organizaciÃ³n, sin frases como 'El nombre oficial es' o explicaciones adicionales. No incluyas ciudades, direcciones ni ubicaciones geogrÃ¡ficas.",
    "Â¿CuÃ¡l es el nÃºmero de NIT o identificaciÃ³n tributaria de la entidad contratante? Responde ÃšNICAMENTE con los nÃºmeros del NIT, sin frases como 'El NIT es' o explicaciones adicionales. Sin espacios ni caracteres especiales.",
    "Â¿CuÃ¡l es la direcciÃ³n fÃ­sica completa de la entidad contratante? Responde ÃšNICAMENTE con la direcciÃ³n postal, sin frases como 'La direcciÃ³n es' o explicaciones adicionales. No incluyas nombres de entidades.",
    "Â¿En quÃ© ciudad estÃ¡ ubicada la entidad contratante? Responde ÃšNICAMENTE con el nombre de la ciudad, sin frases como 'La ciudad es' o explicaciones adicionales.",
    "Â¿CuÃ¡l es el objeto especÃ­fico del contrato, licitaciÃ³n o proceso de compra? Responde ÃšNICAMENTE con la descripciÃ³n del objeto, sin frases como 'El objeto es' o explicaciones adicionales.",
    "Â¿CuÃ¡l es el valor total del contrato, presupuesto o monto estimado? Responde ÃšNICAMENTE con la cifra numÃ©rica y su moneda, sin frases como 'El valor es' o explicaciones adicionales.",
    "Â¿QuÃ© requisitos de experiencia especÃ­fica se mencionan para los proponentes o contratistas? Responde ÃšNICAMENTE con los requisitos, sin frases introductorias o explicaciones adicionales.",
    "Â¿QuÃ© requisitos se mencionan sobre afiliaciÃ³n a salud, pensiÃ³n o seguridad social? Responde ÃšNICAMENTE con los requisitos, sin frases introductorias o explicaciones adicionales.",
    "Â¿QuÃ© documentos anexos, formatos especÃ­ficos o certificados se requieren entregar? Responde ÃšNICAMENTE con la lista de documentos, sin frases introductorias o explicaciones adicionales.",
    "Â¿CuÃ¡l es el cronograma detallado del proceso? Responde ÃšNICAMENTE con las fechas, horarios y actividades tal como aparecen en el documento, sin frases introductorias o explicaciones adicionales."
]

# ConfiguraciÃ³n de OpenAI
openai.api_key = os.getenv('OPENAI_API_KEY')
client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

# ConfiguraciÃ³n para rate limiting
MAX_RETRIES = 5
BASE_DELAY = 1.0
MAX_DELAY = 60.0

# Control inteligente de rate limiting
class RateLimitManager:
    def __init__(self):
        self.requests_per_minute = 0
        self.tokens_per_minute = 0
        self.last_reset = datetime.now()
        self.request_times = []
        self.token_usage = []

        # LÃ­mites conservadores para gpt-4o-mini (Tier 1)
        self.MAX_REQUESTS_PER_MINUTE = 500  # LÃ­mite real es 3000, usamos margen
        self.MAX_TOKENS_PER_MINUTE = 150000  # LÃ­mite real es 200K, usamos margen
        self.PAUSE_THRESHOLD_REQUESTS = 450  # Pausar cuando lleguemos a 450 requests
        self.PAUSE_THRESHOLD_TOKENS = 130000  # Pausar cuando lleguemos a 130K tokens

    def should_pause(self):
        """Determina si debemos hacer una pausa antes de la siguiente request"""
        now = datetime.now()

        # Limpiar requests y tokens de hace mÃ¡s de 1 minuto
        self._cleanup_old_data(now)

        current_requests = len(self.request_times)
        current_tokens = sum(self.token_usage)

        print(f"ğŸ“Š Rate limit actual: {current_requests}/{self.MAX_REQUESTS_PER_MINUTE} requests, {current_tokens}/{self.MAX_TOKENS_PER_MINUTE} tokens")

        # Verificar si estamos cerca de los lÃ­mites
        if current_requests >= self.PAUSE_THRESHOLD_REQUESTS:
            print(f"âš ï¸ Cerca del lÃ­mite de requests ({current_requests}/{self.MAX_REQUESTS_PER_MINUTE})")
            return True, "requests"

        if current_tokens >= self.PAUSE_THRESHOLD_TOKENS:
            print(f"âš ï¸ Cerca del lÃ­mite de tokens ({current_tokens}/{self.MAX_TOKENS_PER_MINUTE})")
            return True, "tokens"

        return False, None

    def _cleanup_old_data(self, now):
        """Limpia datos de hace mÃ¡s de 1 minuto"""
        one_minute_ago = now - timedelta(minutes=1)

        # Filtrar requests antiguos
        self.request_times = [t for t in self.request_times if t > one_minute_ago]

        # Como token_usage corresponde 1:1 con request_times, mantener la misma longitud
        if len(self.token_usage) > len(self.request_times):
            self.token_usage = self.token_usage[-len(self.request_times):]

    def record_request(self, tokens_used=0):
        """Registra una nueva request con su uso de tokens"""
        now = datetime.now()
        self.request_times.append(now)
        self.token_usage.append(tokens_used)

        # Mantener solo el Ãºltimo minuto de datos
        self._cleanup_old_data(now)

    def calculate_pause_time(self, limit_type):
        """Calcula cuÃ¡nto tiempo pausar basado en el tipo de lÃ­mite"""
        now = datetime.now()

        if limit_type == "requests":
            # Encontrar la request mÃ¡s antigua dentro del minuto actual
            if self.request_times:
                oldest_request = min(self.request_times)
                time_until_reset = 60 - (now - oldest_request).total_seconds()
                return max(time_until_reset + 5, 10)  # MÃ­nimo 10 segundos de buffer

        elif limit_type == "tokens":
            # Para tokens, hacer una pausa mÃ¡s conservadora
            return 30  # Pausa fija de 30 segundos

        return 15  # Pausa por defecto

# Instancia global del gestor de rate limiting
rate_limit_manager = RateLimitManager()

async def wait_for_rate_limit(error_message: str, attempt: int) -> float:
    """
    Calcula el tiempo de espera basado en el error de rate limit
    """
    try:
        # Extraer tiempo de espera del mensaje de error si estÃ¡ disponible
        if "Please try again in" in error_message:
            # Buscar patrÃ³n como "Please try again in 11.087s"
            import re
            match = re.search(r'try again in ([\d.]+)s', error_message)
            if match:
                suggested_wait = float(match.group(1))
                # Agregar un poco de buffer adicional
                return min(suggested_wait + random.uniform(1.0, 3.0), MAX_DELAY)
    except:
        pass

    # Fallback: exponential backoff con jitter
    delay = min(BASE_DELAY * (2 ** attempt) + random.uniform(0.1, 1.0), MAX_DELAY)
    return delay

async def call_openai_with_retry(messages: list, max_tokens: int = 500) -> dict:
    """
    Llama a OpenAI con manejo automÃ¡tico de rate limits y reintentos
    """
    # ğŸš€ PAUSA INTELIGENTE: Verificar si debemos pausar antes de hacer la request
    should_pause, limit_type = rate_limit_manager.should_pause()
    if should_pause:
        pause_time = rate_limit_manager.calculate_pause_time(limit_type)
        print(f"ğŸ›‘ PAUSA INTELIGENTE: LÃ­mite de {limit_type} alcanzado")
        print(f"â±ï¸ Pausando {pause_time:.1f} segundos para evitar rate limit...")
        await asyncio.sleep(pause_time)
        print(f"âœ… Pausa completada, reanudando procesamiento...")

    for attempt in range(MAX_RETRIES):
        try:
            print(f"ğŸ¤– Intento {attempt + 1}/{MAX_RETRIES} - Llamada a OpenAI...")

            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                max_tokens=max_tokens,
                temperature=0.3,
                timeout=30.0  # Timeout de 30 segundos
            )

            # ğŸ“Š Registrar la request en el rate limit manager
            tokens_used = response.usage.total_tokens if hasattr(response, 'usage') and response.usage else 0
            rate_limit_manager.record_request(tokens_used)

            return {
                'success': True,
                'response': response,
                'attempt': attempt + 1
            }

        except openai.RateLimitError as e:
            print(f"â³ Rate limit alcanzado en intento {attempt + 1}: {str(e)}")

            if attempt < MAX_RETRIES - 1:
                wait_time = await wait_for_rate_limit(str(e), attempt)
                print(f"â±ï¸ Esperando {wait_time:.1f} segundos antes del siguiente intento...")
                await asyncio.sleep(wait_time)
            else:
                print(f"âŒ MÃ¡ximo de reintentos alcanzado")
                return {
                    'success': False,
                    'error': f'Rate limit despuÃ©s de {MAX_RETRIES} intentos: {str(e)}',
                    'error_type': 'rate_limit'
                }

        except openai.APITimeoutError as e:
            print(f"â° Timeout en intento {attempt + 1}: {str(e)}")

            if attempt < MAX_RETRIES - 1:
                wait_time = min(5.0 * (attempt + 1), 20.0)
                print(f"â±ï¸ Esperando {wait_time:.1f} segundos por timeout...")
                await asyncio.sleep(wait_time)
            else:
                return {
                    'success': False,
                    'error': f'Timeout despuÃ©s de {MAX_RETRIES} intentos: {str(e)}',
                    'error_type': 'timeout'
                }

        except Exception as e:
            print(f"âŒ Error inesperado en intento {attempt + 1}: {str(e)}")

            if attempt < MAX_RETRIES - 1:
                wait_time = min(2.0 * (attempt + 1), 10.0)
                await asyncio.sleep(wait_time)
            else:
                return {
                    'success': False,
                    'error': f'Error despuÃ©s de {MAX_RETRIES} intentos: {str(e)}',
                    'error_type': 'unknown'
                }

    return {
        'success': False,
        'error': 'No se pudo completar la llamada a OpenAI',
        'error_type': 'unknown'
    }

async def analyze_questions_parallel(text_chunks, questions, api_key, max_workers=3):
    """Analiza mÃºltiples preguntas en paralelo - OPTIMIZACIÃ“N PRINCIPAL"""
    print(f"ğŸš€ ANÃLISIS PARALELO: {len(questions)} preguntas con {max_workers} workers")

    if not api_key or api_key == "tu_api_key_aqui":
        return [(f"API Key no configurada", {"tokens_usados": 0, "costo_estimado": 0.0}) for _ in questions]

    # Ejecutar anÃ¡lisis en paralelo
    loop = asyncio.get_event_loop()
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        tasks = [
            loop.run_in_executor(
                executor, 
                analyze_single_question_optimized, 
                text_chunks, 
                question, 
                api_key,
                i+1,
                len(questions)
            ) for i, question in enumerate(questions)
        ]

        results = await asyncio.gather(*tasks)

    print(f"âœ… ANÃLISIS PARALELO COMPLETADO")
    return results

def analyze_single_question_optimized(text_chunks, question, api_key, question_num=1, total_questions=1):
    """Analiza una pregunta con optimizaciones de velocidad"""
    print(f"      ğŸ¤– [{question_num}/{total_questions}] AnÃ¡lisis optimizado...")

    if not api_key:
        return "API Key de OpenAI no configurada", {"tokens_usados": 0, "costo_estimado": 0.0}

    if not question.strip():
        return "Pregunta vacÃ­a", {"tokens_usados": 0, "costo_estimado": 0.0}

    try:
        client = openai.OpenAI(api_key=api_key)
    except Exception as e:
        return f"Error al configurar OpenAI: {str(e)}", {"tokens_usados": 0, "costo_estimado": 0.0}

    # ğŸš€ OPTIMIZACIÃ“N 1: AnÃ¡lisis inteligente de fragmentos
    relevant_chunks = smart_chunk_selection(text_chunks, question)
    print(f"      âš¡ OptimizaciÃ³n: {len(relevant_chunks)}/{len(text_chunks)} fragmentos relevantes")

    all_answers = []
    total_tokens_usados = 0
    total_costo_estimado = 0.0

    # ğŸš€ OPTIMIZACIÃ“N 2: Prompt compacto y eficiente
    prompt_template = get_optimized_prompt_template(question)

    for i, chunk in enumerate(relevant_chunks, 1):
        print(f"         ğŸ“„ Fragmento {i}/{len(relevant_chunks)} ({len(chunk.split())} palabras)...")

        prompt = prompt_template.format(chunk=chunk, question=question)

        try:
            print(f"         ğŸ¤– Consultando GPT-4o-mini con contexto mejorado...")

            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=600,
                temperature=0.0,
                timeout=45
            )

            if hasattr(response, 'usage') and response.usage:
                prompt_tokens = response.usage.prompt_tokens if response.usage else 0
                completion_tokens = response.usage.completion_tokens if response.usage else 0
                total_tokens = response.usage.total_tokens if response.usage else 0

                input_cost = (prompt_tokens / 1_000_000) * PRECIO_INPUT_POR_1M_TOKENS
                output_cost = (completion_tokens / 1_000_000) * PRECIO_OUTPUT_POR_1M_TOKENS
                chunk_cost = input_cost + output_cost

                print(f"         ğŸ’° Tokens: entrada={prompt_tokens}, salida={completion_tokens}, total={total_tokens}")
                print(f"         ğŸ’¸ Costo estimado: ${chunk_cost:.4f}")

                total_tokens_usados += total_tokens
                total_costo_estimado += chunk_cost
            else:
                print("         ğŸ’° Tokens: No se pudo calcular el uso de tokens")
                total_costo_estimado = 0
                total_tokens_usados = 0

            answer = response.choices[0].message.content.strip()
            print(f"         ğŸ“ Respuesta: {answer[:80]}{'...' if len(answer) > 80 else ''}")

            if answer and len(answer.strip()) > 3:
                # ValidaciÃ³n bÃ¡sica menos restrictiva
                respuestas_invalidas = [
                    "no encontrado", "no se encontrÃ³", "no aparece", "no estÃ¡ disponible",
                    "no mencionado", "no especificado", "sin informaciÃ³n"
                ]

                # Solo rechazar si la respuesta es claramente invÃ¡lida
                if not any(invalida in answer.lower() for invalida in respuestas_invalidas):
                    all_answers.append(answer)
                    print(f"         âœ… Respuesta encontrada en fragmento {i}")
                else:
                    print(f"         âš ï¸ Respuesta indica que no se encontrÃ³ informaciÃ³n")
            else:
                print(f"         âŒ Respuesta muy corta o vacÃ­a")

        except Exception as e:
            error_msg = f"Error en fragmento {i+1}: {str(e)}"
            print(f"         âŒ {error_msg}")
            continue

    print(f"      ğŸ“‹ Completado: {len(all_answers)} respuestas vÃ¡lidas de {len(text_chunks)} fragmentos")
    print(f"      ğŸ’° Total tokens usados: {total_tokens_usados} | Costo total: ${total_costo_estimado:.4f}")

    metricas = {
        "tokens_usados": total_tokens_usados,
        "costo_estimado": total_costo_estimado,
        "fragmentos_procesados": len(text_chunks),
        "respuestas_encontradas": len(all_answers)
    }

    if all_answers:
        if len(all_answers) == 1:
            final_answer = all_answers[0]
        else:
            sorted_answers = sorted(all_answers, key=len, reverse=True)
            final_answer = sorted_answers[0]

            unique_answers = []
            for ans in sorted_answers:
                if not any(ans.lower() in existing.lower() or existing.lower() in ans.lower() 
                          for existing in unique_answers):
                    unique_answers.append(ans)

            if len(unique_answers) > 1:
                final_answer = " | ".join(unique_answers[:2])
            else:
                final_answer = unique_answers[0] if unique_answers else sorted_answers[0]

        print(f"      âœ… Respuesta final validada: {final_answer[:100]}{'...' if len(final_answer) > 100 else ''}")
        return final_answer, metricas
    else:
        print(f"      âŒ No se encontrÃ³ informaciÃ³n especÃ­fica vÃ¡lida")
        return "No se encontrÃ³ informaciÃ³n especÃ­fica para esta pregunta", metricas

def process_custom_questions(preguntas_personalizadas):
    """Procesa preguntas personalizadas del usuario"""
    preguntas_finales = DEFAULT_QUESTIONS.copy()
    if preguntas_personalizadas:
        try:
            preguntas_custom = json.loads(preguntas_personalizadas)
            if isinstance(preguntas_custom, list):
                preguntas_finales.extend(preguntas_custom)
                print(f"âœ… Se agregaron {len(preguntas_custom)} preguntas personalizadas")
        except json.JSONDecodeError:
            print(f"âš ï¸ Error en formato JSON de preguntas personalizadas - usando preguntas por defecto")

    return preguntas_finales

def smart_chunk_selection(text_chunks, question):
    """SelecciÃ³n inteligente de fragmentos relevantes para la pregunta"""
    # Palabras clave por tipo de pregunta
    keywords_map = {
        'entidad': ['entidad', 'instituciÃ³n', 'ministerio', 'alcaldÃ­a', 'gobernaciÃ³n', 'empresa', 'contratante'],
        'nit': ['nit', 'identificaciÃ³n', 'tributaria', 'rut'],
        'ciudad': ['ciudad', 'municipio', 'sede', 'ubicaciÃ³n'],
        'direccion': ['direcciÃ³n', 'direcciÃ³n', 'calle', 'carrera', 'avenida'],
        'valor': ['valor', 'presupuesto', 'precio', 'costo', '$', 'pesos', 'millones'],
        'cronograma': ['fecha', 'plazo', 'tÃ©rmino', 'cronograma', 'tiempo'],
        'experiencia': ['experiencia', 'requisitos', 'aÃ±os', 'similar'],
        'salud': ['salud', 'pensiÃ³n', 'seguridad social', 'afiliaciÃ³n'],
        'anexos': ['anexos', 'formatos', 'documentos', 'certificado']
    }

    question_lower = question.lower()
    relevant_keywords = []

    # Identificar tipo de pregunta
    for category, keywords in keywords_map.items():
        if any(keyword in question_lower for keyword in keywords):
            relevant_keywords.extend(keywords)
            break

    if not relevant_keywords:
        # Si no se identifica el tipo, usar todos los fragmentos
        return text_chunks[:3]  # MÃ¡ximo 3 fragmentos

    # Puntuar fragmentos por relevancia
    scored_chunks = []
    for chunk in text_chunks:
        chunk_lower = chunk.lower()
        score = sum(1 for keyword in relevant_keywords if keyword in chunk_lower)
        if score > 0:
            scored_chunks.append((chunk, score))

    # Ordenar por relevancia y tomar los mejores
    scored_chunks.sort(key=lambda x: x[1], reverse=True)

    # Retornar mÃ¡ximo 2-3 fragmentos mÃ¡s relevantes
    max_chunks = 2 if len(scored_chunks) > 5 else 3
    return [chunk for chunk, score in scored_chunks[:max_chunks]]

def estimate_tokens(text: str) -> int:
    """
    Estima la cantidad de tokens de un texto (aproximaciÃ³n)
    """
    # AproximaciÃ³n: 1 token â‰ˆ 4 caracteres en inglÃ©s/espaÃ±ol
    return len(text) // 3  # Usar 3 para ser conservadores

def check_token_limits(messages: list, max_tokens: int = 500) -> bool:
    """
    Verifica si la llamada estÃ¡ dentro de los lÃ­mites de tokens
    """
    total_input_tokens = sum(estimate_tokens(msg.get('content', '')) for msg in messages)

    # LÃ­mite conservador para gpt-4o-mini
    max_input_tokens = 100000  # LÃ­mite real es 128k, usar margen de seguridad

    if total_input_tokens + max_tokens > max_input_tokens:
        print(f"âš ï¸ Advertencia: Tokens estimados ({total_input_tokens + max_tokens}) cerca del lÃ­mite")
        return False

    return True

def get_optimized_system_prompt(question: str) -> str:
    """Genera prompts optimizados segÃºn el tipo de pregunta"""
    question_lower = question.lower()

    system_prompt = """
Eres un asistente especializado en anÃ¡lisis de documentos de contrataciÃ³n pÃºblica colombiana.

INSTRUCCIONES CRÃTICAS:
1. Analiza EXHAUSTIVAMENTE el texto proporcionado
2. Responde ÃšNICAMENTE basÃ¡ndote en la informaciÃ³n encontrada en el texto
3. Si la informaciÃ³n no estÃ¡ en el texto, responde: "No encontrado"
4. NO inventes, supongas o agregues informaciÃ³n
5. Extrae informaciÃ³n EXACTAMENTE como aparece en el documento
6. Busca variaciones de la informaciÃ³n solicitada (sinÃ³nimos, diferentes formatos)

FORMATO DE RESPUESTA REQUERIDO:
- Responde ÃšNICAMENTE con la informaciÃ³n solicitada, SIN frases introductorias
- NO uses frases como "El nombre es", "La direcciÃ³n es", "El valor es", "segÃºn el documento", "el texto indica", etc.
- NO agregues explicaciones adicionales ni comentarios
- Ve directamente al grano con SOLO la informaciÃ³n especÃ­fica solicitada
- Ejemplo: Si preguntan por el NIT, responde solo "800123456", NO "El NIT es 800123456"
"""

    # Prompt general mÃ¡s permisivo que funcionaba antes
    return system_prompt + """\n\nTEXTO DEL DOCUMENTO:\n{chunk}\n\nPREGUNTA: {question}\n\nRESPUESTA:"""

async def analyze_single_question(text_fragments: List[str], question: str, question_number: int) -> Dict[str, Any]:
    """
    Analiza una pregunta especÃ­fica contra los fragmentos de texto usando OpenAI
    """
    start_time = time.time()

    try:
        print(f"ğŸ¤– Pregunta {question_number}: {question[:80]}...")

        # Combinar fragmentos relevantes
        combined_text = "\n\n".join(text_fragments[:5])  # Limitar a 5 fragmentos

        # Crear prompt optimizado
        messages = [
            {
                "role": "system",
                "content": get_optimized_system_prompt(question)
            },
            {
                "role": "user", 
                "content": f"""Analiza este documento y responde la pregunta especÃ­fica:

DOCUMENTO:
{combined_text}

PREGUNTA: {question}

Instrucciones:
- Responde SOLO lo que se pregunta
- Si no encuentras informaciÃ³n especÃ­fica, responde "No se encontrÃ³ informaciÃ³n especÃ­fica"
- SÃ© conciso y directo
- No agregues explicaciones extra"""
            }
        ]

        # Llamar a OpenAI con retry logic
        openai_result = await call_openai_with_retry(messages, max_tokens=500)

        if not openai_result['success']:
            # Manejar errores especÃ­ficos
            error_type = openai_result.get('error_type', 'unknown')
            error_msg = openai_result.get('error', 'Error desconocido')

            print(f"âŒ Error en pregunta {question_number}: {error_msg}")

            return {
                "pregunta_numero": question_number,
                "pregunta": question,
                "respuesta": f"Error en anÃ¡lisis: {error_type}",
                "informacion_encontrada": False,
                "fragmentos_analizados": len(text_fragments),
                "tiempo_procesamiento": time.time() - start_time,
                "metricas_openai": {
                    "error": error_msg,
                    "error_type": error_type,
                    "intentos_realizados": openai_result.get('attempt', 0)
                }
            }

        response = openai_result['response']

        # Procesar la respuesta de OpenAI
        respuesta = response.choices[0].message.content.strip()
        informacion_encontrada = respuesta.lower() != "no se encontrÃ³ informaciÃ³n especÃ­fica"

        print(f"âœ… Pregunta {question_number} completada")

        return {
            "pregunta_numero": question_number,
            "pregunta": question,
            "respuesta": respuesta,
            "informacion_encontrada": informacion_encontrada,
            "fragmentos_analizados": len(text_fragments),
            "tiempo_procesamiento": time.time() - start_time,
            "metricas_openai": {
                "tokens_usados": response.usage.total_tokens,
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "costo_estimado": (response.usage.total_tokens / 1000000) * (PRECIO_INPUT_POR_1M_TOKENS + PRECIO_OUTPUT_POR_1M_TOKENS),
                "intentos_realizados": openai_result.get('attempt', 1)
            }
        }

    except Exception as e:
        print(f"âŒ Error inesperado en pregunta {question_number}: {str(e)}")
        return {
            "pregunta_numero": question_number,
            "pregunta": question,
            "respuesta": f"Error inesperado: {str(e)}",
            "informacion_encontrada": False,
            "fragmentos_analizados": len(text_fragments),
            "tiempo_procesamiento": time.time() - start_time,
            "metricas_openai": {
                "error": str(e),
                "tokens_usados": 0,
                "costo_estimado": 0
            }
        }

async def analyze_with_ai_parallel(text_fragments: List[str], custom_questions: List[str] = None) -> List[Dict[str, Any]]:
    """
    Analiza fragmentos de texto usando IA de forma secuencial con delays para evitar rate limits
    """
    print(f"ğŸ¤– Iniciando anÃ¡lisis con IA - {len(text_fragments)} fragmentos")

    questions = custom_questions if custom_questions else DEFAULT_QUESTIONS
    print(f"ğŸ“ Analizando {len(questions)} preguntas")

    # Procesar preguntas de forma secuencial para evitar rate limits
    results = []
    start_time = time.time()

    for i, question in enumerate(questions):
        print(f"ğŸ“‹ Procesando pregunta {i + 1}/{len(questions)}")

        try:
            # ğŸš€ VERIFICACIÃ“N INTELIGENTE antes de cada pregunta
            should_pause, limit_type = rate_limit_manager.should_pause()
            if should_pause:
                pause_time = rate_limit_manager.calculate_pause_time(limit_type)
                print(f"ğŸ›‘ PAUSA INTELIGENTE antes de pregunta {i + 1}")
                print(f"â±ï¸ LÃ­mite de {limit_type} alcanzado, pausando {pause_time:.1f} segundos...")
                await asyncio.sleep(pause_time)
                print(f"âœ… Pausa completada, procesando pregunta {i + 1}")

            result = await analyze_single_question(text_fragments, question, i + 1)
            results.append(result)

            # Delay adaptativo basado en el estado del rate limit
            if i < len(questions) - 1:  # No delay despuÃ©s de la Ãºltima pregunta
                # Delay mÃ¡s corto si estamos lejos de los lÃ­mites, mÃ¡s largo si estamos cerca
                current_requests = len(rate_limit_manager.request_times)
                current_tokens = sum(rate_limit_manager.token_usage)

                if current_requests > 300 or current_tokens > 100000:
                    delay = random.uniform(3.0, 5.0)  # Delay mÃ¡s largo si estamos cerca de lÃ­mites
                    print(f"â±ï¸ Delay extendido: {delay:.1f}s (cerca de lÃ­mites)")
                else:
                    delay = random.uniform(1.0, 2.5)  # Delay normal
                    print(f"â±ï¸ Delay normal: {delay:.1f}s")

                await asyncio.sleep(delay)

        except Exception as e:
            print(f"âŒ Error inesperado en pregunta {i + 1}: {str(e)}")
            results.append({
                "pregunta_numero": i + 1,
                "pregunta": question,
                "respuesta": f"Error crÃ­tico en procesamiento: {str(e)}",
                "informacion_encontrada": False,
                "fragmentos_analizados": len(text_fragments),
                "tiempo_procesamiento": 0,
                "metricas_openai": {
                    "error": str(e),
                    "tokens_usados": 0,
                    "costo_estimado": 0
                }
            })

    total_time = time.time() - start_time

    # ğŸ“Š InformaciÃ³n final del rate limiting
    final_requests = len(rate_limit_manager.request_times)
    final_tokens = sum(rate_limit_manager.token_usage)

    print(f"âœ… AnÃ¡lisis completado en {total_time:.2f} segundos")
    print(f"ğŸ“Š Rate limiting final: {final_requests}/{rate_limit_manager.MAX_REQUESTS_PER_MINUTE} requests, {final_tokens}/{rate_limit_manager.MAX_TOKENS_PER_MINUTE} tokens")

    # Agregar informaciÃ³n de rate limiting a los resultados
    for result in results:
        if 'metricas_openai' in result:
            result['metricas_openai']['rate_limit_info'] = {
                'requests_utilizados': final_requests,
                'tokens_utilizados': final_tokens,
                'limite_requests': rate_limit_manager.MAX_REQUESTS_PER_MINUTE,
                'limite_tokens': rate_limit_manager.MAX_TOKENS_PER_MINUTE
            }

    return results